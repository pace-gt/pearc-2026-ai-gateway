Bootstrap: docker
From: nvcr.io/nvidia/tritonserver:24.08-py3

%files
    backend/diffusion/model.py /opt/tritonserver/backends/diffusion/model.py
    diffusion-models /workspace/diffusion-models
    scripts/build_models.py /workspace/scripts/build_models.py

%post
    # Use /tmp which should have more space in the build environment
    export PIP_CACHE_DIR=/tmp/pip-cache
    export PIP_NO_CACHE_DIR=0
    export TMPDIR=/tmp
    export TEMP=/tmp
    export TMP=/tmp
    mkdir -p /tmp/pip-cache

    # Clean up any existing large packages to free space
    apt-get clean
    rm -rf /var/lib/apt/lists/*

    echo "=== Installing TensorRT 10.4.0 ==="
    pip install --no-cache-dir --pre --upgrade --extra-index-url https://pypi.nvidia.com tensorrt-cu12==10.4.0

    # Clean pip cache after each major install to save space
    rm -rf /tmp/pip-cache/*

    echo "=== Cloning TensorRT repository ==="
    rm -rf /tmp/TensorRT
    git clone https://github.com/NVIDIA/TensorRT.git -b release/10.4 --single-branch /tmp/TensorRT

    echo "=== Installing PyTorch 2.5.1 (CUDA 12.1 wheels) ==="
    pip3 install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 \
      torch==2.5.1 torchvision==0.20.1

    rm -rf /tmp/pip-cache/*

    echo "=== Installing CUDA Python bindings ==="
    # Install specific CUDA bindings that work with CUDA 12.x
    pip3 install --no-cache-dir --extra-index-url https://pypi.nvidia.com \
      cuda-python~=12.0

    echo "=== Installing core dependencies ==="
    pip3 install --no-cache-dir \
      colored \
      controlnet-aux==0.0.6 \
      diffusers==0.29.2 \
      ftfy==6.3.1 \
      matplotlib \
      nvtx \
      onnx==1.15.0 \
      onnxruntime==1.17.3 \
      opencv-python==4.8.0.74 \
      scipy \
      transformers==4.36.2 \
      sentencepiece==0.2.1

    rm -rf /tmp/pip-cache/*

    echo "=== Installing ModelOpt with torch and onnx extras ==="
    pip3 install --no-cache-dir --extra-index-url https://pypi.nvidia.com \
      'nvidia-modelopt[torch,onnx]==0.15.1'

    rm -rf /tmp/pip-cache/*

    echo "=== Installing ONNX and TensorRT tools ==="
    pip3 install --no-cache-dir --extra-index-url https://pypi.nvidia.com \
      onnx-graphsurgeon \
      onnxmltools==1.11.0 \
      onnxconverter-common==1.13.0 \
      polygraphy==0.49.9

    echo "=== Installing Triton client ==="
    pip3 install --no-cache-dir tritonclient[all]

    echo "=== Installing additional dependencies ==="
    pip3 install --no-cache-dir \
      accelerate \
      einops \
      timm \
      tokenizers \
      safetensors \
      huggingface-hub

    echo "=== Installing FastAPI and API dependencies ==="
    pip3 install --no-cache-dir \
      fastapi \
      pydantic \
      pillow \
      uvicorn

    rm -rf /tmp/pip-cache/*

    echo "=== Setting up backend directory structure ==="
    mkdir -p /opt/tritonserver/backends/diffusion

    echo "=== Copying TensorRT Diffusion demo to backend ==="
    cp -rf /tmp/TensorRT/demo/Diffusion /opt/tritonserver/backends/diffusion/

    echo "=== Installing tritonserver in-process API ==="
    find /opt/tritonserver/python -maxdepth 1 -type f -name \
         "tritonserver-*.whl" | xargs -I {} pip3 install --upgrade {}[all]

    echo "=== Verifying CUDA Python installation ==="
    python3 -c "from cuda import cudart; print('CUDA Runtime bindings OK')" || \
    python3 -c "from cuda.bindings import runtime as cudart; print('CUDA Runtime bindings OK (new path)')"

    echo "=== Creating wrapper script for build_models ==="
    mkdir -p /workspace/scripts
    cat > /workspace/scripts/container_build_models.sh << 'EOF'
#!/bin/bash -e
# Wrapper script to run build_models.py directly without reinstalling tritonserver

SOURCE_DIR=$(dirname "$(readlink -f "$0")")

# Ensure we're using container's Python
export PATH=/usr/local/bin:/usr/bin:/bin:$PATH
export PYTHONPATH=/opt/tritonserver/backends/diffusion:$PYTHONPATH

# Use container's libraries, but allow PyTorch's bundled CUDA runtime
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/tritonserver/lib:$LD_LIBRARY_PATH

# Run python script directly
python3 $SOURCE_DIR/build_models.py "$@"
EOF
    chmod +x /workspace/scripts/container_build_models.sh

    echo "=== Cleaning up build artifacts ==="
    rm -rf /tmp/TensorRT/.git
    rm -rf /tmp/pip-cache
    apt-get clean
    rm -rf /var/lib/apt/lists/*

    echo "=== Build complete ==="

%environment
    export HF_TOKEN=${HF_TOKEN:-}
    export HF_HOME=/workspace/.cache/huggingface
    export HUGGINGFACE_HUB_CACHE=/workspace/.cache/huggingface
    export TRANSFORMERS_CACHE=/workspace/.cache/huggingface

    # Ensure container's Python is first in PATH
    export PATH=/usr/local/bin:/usr/bin:/bin:$PATH
    export PYTHONPATH=/opt/tritonserver/backends/diffusion:$PYTHONPATH

    # CUDA paths - container's CUDA first, then let Apptainer's --nv add host paths
    export CUDA_HOME=/usr/local/cuda
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/tritonserver/lib:${LD_LIBRARY_PATH}

%runscript
    exec "$@"

%labels
    Author NVIDIA Corporation & AFFILIATES
    Version 24.08
    PyTorch 2.5.1
    TensorRT 10.4.0
    Framework Diffusion
    Description Triton Inference Server with Stable Diffusion support
